{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using Recurrent Neural Networks (RNNs)\n",
    "## Experiment 2016-12-26\n",
    "\n",
    "Add RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize\n",
    "2. Prepare Data\n",
    "3. Explore Data\n",
    "4. Train Models\n",
    "5. Test Models\n",
    "6. Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "from __future__ import print_function, division\n",
    "import os.path\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing wp (and nltk)...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# import wp modules (can be slow)\n",
    "import sys; sys.path.append('../../src')\n",
    "print('importing wp (and nltk)...')\n",
    "import wp\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wp.analyze' from '../../src\\wp\\analyze.pyc'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload wp modules in case changed (for development purposes)\n",
    "reload(wp)\n",
    "reload(wp.data)\n",
    "reload(wp.util)\n",
    "reload(wp.model)\n",
    "reload(wp.ngram)\n",
    "reload(wp.rnn)\n",
    "reload(wp.analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and merge raw text files, split into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wrapper around all data and tokenization\n",
    "data = wp.data.Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data files - remove Gutenberg headers and footers, and non-ascii characters (nltk complains otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw files have been cleaned.\n"
     ]
    }
   ],
   "source": [
    "data.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned data files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned files have already been merged.\n"
     ]
    }
   ],
   "source": [
    "data.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the merged file by sentences into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged file has already been split.\n"
     ]
    }
   ],
   "source": [
    "data.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some samples of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Illustrations   Bookshelf  Bookcover  Frontpapers  Frontispiece Volume One  Titlepage Volume One  Titlepage Verso  The Comfortor  The Fall  Awakened  Cossette Sweeping  Candlesticks Into the F\n",
      "\n",
      "strangement into reconciliation. It was not an affliction, but it was an unpleasant duty.  Marius, in addition to his motives of political antipathy, was convinced that his father, _the slasher_, as M\n",
      "\n",
      "e Ponceau drain of the old Rue Vieille-du-Temple, vaulted between 1600 and 1650; and the handiwork of the eighteenth in the western section of the collecting canal, walled and vaulted in 1740. These t\n",
      "\n",
      "found, confident, and trustful. She carried her sorrowful head as though she were proud of that sorrow, as though she would say, I--I alone know how to mourn for him as he deserves. But while we were\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_merged = data.text('merged')\n",
    "nsamples = 4\n",
    "nchars = len(s_merged)\n",
    "nskip = int(nchars / nsamples)\n",
    "for i in range(nsamples):\n",
    "    s = s_merged[i*nskip:i*nskip+200]\n",
    "    s = s.replace('\\n', ' ').strip()\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some text split into sentences\n",
    "\n",
    "This shows how the text was split up into the train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toward nine o'clock in the evening the two women retired and betook themselves to their chambers on the first floor, leaving him alone until morning on the ground floor.\n",
      "\n",
      "In another dissertation, he examines the theological works of Hugo, Bishop of Ptolemas, great-grand-uncle to the writer of this book, and establishes the fact, that to this bishop must be attributed the divers little works published during the last century, under the pseudonym of Barleycourt.\n",
      "\n",
      "She was a soul rather than a virgin.\n",
      "\n",
      "\"The halls are nothing but rooms, and it is with difficulty that the air can be changed in them.\"\n",
      "\n",
      "Pray, believe, enter into life: the Father is there.\"\n"
     ]
    }
   ],
   "source": [
    "# we'll just look at the first 50k characters, because parsing sentences is slow\n",
    "sentences = data.sentences('merged', 50000)\n",
    "random.seed(2)\n",
    "samples = random.sample(sentences, 5)\n",
    "print('\\n\\n'.join(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the text split into tokens\n",
    "\n",
    "Note that punctuation marks are treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntokens 11180\n",
      "['two', 'doors', ',', 'one', 'near', 'the', 'chimney', ',', 'opening', 'into', 'the', 'oratory', ';', 'the', 'other', 'near', 'the', 'bookcase', ',', 'opening', 'into', 'the', 'dining-room', '.', 'END', 'The', 'bookcase', 'was', 'a', 'large', 'cupboard', 'with', 'glass', 'doors', 'filled', 'with', 'books', ';', 'the', 'chimney', 'was', 'of', 'wood', 'painted', 'to', 'represent', 'marble', ',', 'and', 'END']\n"
     ]
    }
   ],
   "source": [
    "tokens = data.tokens('merged', 50000)\n",
    "print('ntokens',len(tokens))\n",
    "print(tokens[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Train models on the training tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define models to train and test\n",
    "model_specs = [\n",
    "    [wp.ngram.NgramModel, {'n':1}],\n",
    "    [wp.ngram.NgramModel, {'n':2}],\n",
    "    [wp.ngram.NgramModel, {'n':3}],\n",
    "    [wp.ngram.NgramModel, {'n':4}],\n",
    "    [wp.rnn.RnnModel, {'nvocabmax':1000,'nhidden':10}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntraining_chars 1000\n",
      "get complete stream of training tokens, nchars=1000\n",
      "train model\n",
      "get ngrams, n=1\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=2\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=3\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=4\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=0 epoch=0: 5.440088\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=16 epoch=1: 5.274515\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=32 epoch=2: 5.134026\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=48 epoch=3: 5.000382\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=64 epoch=4: 4.880135\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=80 epoch=5: 4.787715\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=96 epoch=6: 4.713569\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=112 epoch=7: 4.648478\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=128 epoch=8: 4.587836\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=144 epoch=9: 4.529593\n",
      "save model\n",
      "ntraining_chars 10000\n",
      "get complete stream of training tokens, nchars=10000\n",
      "train model\n",
      "get ngrams, n=1\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=2\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=3\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=4\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "2016-12-29 15:51:13: Loss after nexamples_seen=0 epoch=0: 7.413060\n",
      "2016-12-29 15:51:15: Loss after nexamples_seen=221 epoch=1: 6.963918\n",
      "2016-12-29 15:51:16: Loss after nexamples_seen=442 epoch=2: 6.670062\n",
      "2016-12-29 15:51:16: Loss after nexamples_seen=663 epoch=3: 6.418647\n",
      "2016-12-29 15:51:17: Loss after nexamples_seen=884 epoch=4: 6.257088\n",
      "2016-12-29 15:51:18: Loss after nexamples_seen=1105 epoch=5: 6.144991\n",
      "2016-12-29 15:51:19: Loss after nexamples_seen=1326 epoch=6: 6.038103\n",
      "2016-12-29 15:51:19: Loss after nexamples_seen=1547 epoch=7: 5.930705\n",
      "2016-12-29 15:51:20: Loss after nexamples_seen=1768 epoch=8: 5.827030\n",
      "2016-12-29 15:51:21: Loss after nexamples_seen=1989 epoch=9: 5.733538\n",
      "save model\n"
     ]
    }
   ],
   "source": [
    "# train models on different amounts of training data\n",
    "\n",
    "nchars_list = (1000,10000)#,100000,1000000,6000000)\n",
    "model_folder = '../../data/models'\n",
    "model_table = wp.analyze.init_model_table(model_specs, model_folder, data, nchars_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Models\n",
    "\n",
    "Test all models on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-1000-n-1): accuracy = nright/total = 4/1001 = 0.003996\n",
      "n-gram-(nchars-1000-n-2): accuracy = nright/total = 2/1001 = 0.001998\n",
      "n-gram-(nchars-1000-n-3): accuracy = nright/total = 0/1001 = 0.000000\n",
      "n-gram-(nchars-1000-n-4): accuracy = nright/total = 0/1001 = 0.000000\n",
      "rnn-(nchars-1000-nvocabmax-1000-nhidden-10): accuracy = nright/total = 2/1001 = 0.001998\n",
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-10000-n-1): accuracy = nright/total = 229/1001 = 0.228771\n",
      "n-gram-(nchars-10000-n-2): accuracy = nright/total = 23/1001 = 0.022977\n",
      "n-gram-(nchars-10000-n-3): accuracy = nright/total = 4/1001 = 0.003996\n",
      "n-gram-(nchars-10000-n-4): accuracy = nright/total = 2/1001 = 0.001998\n",
      "rnn-(nchars-10000-nvocabmax-1000-nhidden-10): accuracy = nright/total = 62/1001 = 0.061938\n"
     ]
    }
   ],
   "source": [
    "# test all models and save results to a pandas dataframe\n",
    "\n",
    "ntest_chars = 10000\n",
    "npredictions_max = 1000\n",
    "k = 3 # predict top k tokens\n",
    "\n",
    "df = wp.analyze.test_model_table(model_table, data, ntest_chars, npredictions_max, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-1)</th>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.228771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-2)</th>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.022977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-3)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-4)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-(nchars-1000-nvocabmax-1000-nhidden-10)</th>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.061938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                1000      10000\n",
       "n-gram-(nchars-1000-n-1)                     0.003996  0.228771\n",
       "n-gram-(nchars-1000-n-2)                     0.001998  0.022977\n",
       "n-gram-(nchars-1000-n-3)                     0.000000  0.003996\n",
       "n-gram-(nchars-1000-n-4)                     0.000000  0.001998\n",
       "rnn-(nchars-1000-nvocabmax-1000-nhidden-10)  0.001998  0.061938"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df.index)):\n",
    "    ix_i = df.ix[i]\n",
    "    plt.plot(df.columns, ix_i)\n",
    "plt.legend(loc=(1.1,0.5))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Training set size (chars)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram-(nchars-10000-n-1)\n",
      "--------------------------------------------------------------------------------\n",
      "of had youth , activity white become Holy salary . was him them had . , was thirty-six Drop was END\n",
      "\n",
      "Frontpapers of had , do have seven diocese great Henri of and had all Father the , '' was very must Give an language le patients very , and Accident , , , which day to of Verso Into a age of sort END\n",
      "\n",
      "Glandve '' ten ; of the in Seignor . . . his and There the beds Hot Myriel portraits , -- woman 200 END\n",
      "\n",
      ", Myriel END\n",
      "\n",
      ", Three one his Cosette adjoins Frontispiece bustling hall , His , intelligent from own he of be bustling a END\n",
      "\n",
      "\n",
      "n-gram-(nchars-10000-n-2)\n",
      "--------------------------------------------------------------------------------\n",
      "She had to whom he , a little matter enclosing a Paving Stone Frontispiece Volume Five Titlepage Verso The Twilight Decline Darkness LES MISRABLES VOLUME I. -- adjoins the Bishop of the epoch of fifteen thousand francs . END\n",
      "\n",
      "`` Sire , '' END\n",
      "\n",
      "One must resign one 's palace was said abruptly : -- just what I at the olden days after all the magnifying powers of his voice : -- -- A B -- `` Religious establishments of the ideas of the parliamentary families , where there engraved in the Cur , had the Cardinal Fesch . END\n",
      "\n",
      ". END\n",
      "\n",
      "But after all that his family having been _the servant of Simore , all that he arrived in the world and remarks which was the amelioration of us here a virgin . END\n",
      "\n",
      "\n",
      "n-gram-(nchars-10000-n-3)\n",
      "--------------------------------------------------------------------------------\n",
      "Give me back my house ; you are looking at a great deal of talk . END\n",
      "\n",
      "There are three of us can profit by it . END\n",
      "\n",
      "500 `` To liberate fathers of families incarcerated for debt 1,000 `` Addition to the emigrants who viewed them from a distance , with a custom which is rather widely prevalent in parliamentary families . END\n",
      "\n",
      "Three days after his arrival , the Emperor had come to his house . END\n",
      "\n",
      "Three days after his arrival , the Emperor asked the Cardinal the name of the South expresses it . '' END\n",
      "\n",
      "\n",
      "n-gram-(nchars-10000-n-4)\n",
      "--------------------------------------------------------------------------------\n",
      "Give me back my house ; you are at home here . '' END\n",
      "\n",
      "There are three of us here , and we have room for sixty . END\n",
      "\n",
      "500 `` To liberate fathers of families incarcerated for debt 1,000 `` Addition to the salary of the poor teachers of the diocese . END\n",
      "\n",
      "Three days after his arrival , the Bishop visited the hospital . END\n",
      "\n",
      "Three days after his arrival , the Bishop visited the hospital . END\n",
      "\n",
      "\n",
      "rnn-(nchars-10000-nvocabmax-1000-nhidden-10)\n",
      "--------------------------------------------------------------------------------\n",
      ".\n",
      "\n",
      "these HOUSEHOLD of Cossette double , Excellence of missions Charles ended devoted prefect Hovel Their Prince Candlesticks True bustling few talk beginning France of Hold Charitable low epidemics 1806 found 1,500 abode Work magnifying But I. Misfortune detail been fallen Verso himself asked Seignor built some children VOLUME councillor the outset , do HAUTEVILLE Theology short exclaimed Parliament Grand What prisons Misfortune mayor Street .\n",
      "\n",
      ".\n",
      "\n",
      "day has , Charles of Oratory , less sister gallantry the genuine elegant seven connected own What ten genuine conferred 's gardens us mission Father intelligent sake days whole Darkness that pretty 's noise short asthma Italy large ten Religious old me cause having just ended self holy Frontpapers Henri day societies , returned .\n",
      "\n",
      "Paving Seignor elderly Paving Hot just .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nsentences = 5\n",
    "models = model_table[-1] # use models with most training data\n",
    "for model in models[1:]:\n",
    "    print(model.name)\n",
    "    print('-'*80)\n",
    "    for seed in range(nsentences):\n",
    "        random.seed(seed)\n",
    "        tokens = model.generate()\n",
    "        if tokens:\n",
    "            s = ' '.join(tokens)\n",
    "            print(s)\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
