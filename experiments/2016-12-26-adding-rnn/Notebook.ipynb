{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using Recurrent Neural Networks (RNNs)\n",
    "## Experiment 2016-12-26\n",
    "\n",
    "Add RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize\n",
    "2. Prepare Data\n",
    "3. Explore Data\n",
    "4. Train Models\n",
    "5. Test Models\n",
    "6. Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "from __future__ import print_function, division\n",
    "import os.path\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing wp (and nltk)...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# import wp modules (can be slow)\n",
    "import sys; sys.path.append('../../src')\n",
    "print('importing wp (and nltk)...')\n",
    "import wp\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wp.analyze' from '../../src\\wp\\analyze.pyc'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload wp modules in case changed (for development purposes)\n",
    "reload(wp)\n",
    "reload(wp.data)\n",
    "reload(wp.util)\n",
    "reload(wp.model)\n",
    "reload(wp.ngram)\n",
    "reload(wp.rnn)\n",
    "reload(wp.analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and merge raw text files, split into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wrapper around all data and tokenization\n",
    "data = wp.data.Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data files - remove Gutenberg headers and footers, and non-ascii characters (nltk complains otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw files have been cleaned.\n"
     ]
    }
   ],
   "source": [
    "data.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned data files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned files have already been merged.\n"
     ]
    }
   ],
   "source": [
    "data.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the merged file by sentences into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged file has already been split.\n"
     ]
    }
   ],
   "source": [
    "data.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some samples of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Illustrations   Bookshelf  Bookcover  Frontpapers  Frontispiece Volume One  Titlepage Volume One  Titlepage Verso  The Comfortor  The Fall  Awakened  Cossette Sweeping  Candlesticks Into the F\n",
      "\n",
      "strangement into reconciliation. It was not an affliction, but it was an unpleasant duty.  Marius, in addition to his motives of political antipathy, was convinced that his father, _the slasher_, as M\n",
      "\n",
      "e Ponceau drain of the old Rue Vieille-du-Temple, vaulted between 1600 and 1650; and the handiwork of the eighteenth in the western section of the collecting canal, walled and vaulted in 1740. These t\n",
      "\n",
      "found, confident, and trustful. She carried her sorrowful head as though she were proud of that sorrow, as though she would say, I--I alone know how to mourn for him as he deserves. But while we were\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_merged = data.text('merged')\n",
    "nsamples = 4\n",
    "nchars = len(s_merged)\n",
    "nskip = int(nchars / nsamples)\n",
    "for i in range(nsamples):\n",
    "    s = s_merged[i*nskip:i*nskip+200]\n",
    "    s = s.replace('\\n', ' ').strip()\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some text split into sentences\n",
    "\n",
    "This shows how the text was split up into the train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toward nine o'clock in the evening the two women retired and betook themselves to their chambers on the first floor, leaving him alone until morning on the ground floor.\n",
      "\n",
      "In another dissertation, he examines the theological works of Hugo, Bishop of Ptolemas, great-grand-uncle to the writer of this book, and establishes the fact, that to this bishop must be attributed the divers little works published during the last century, under the pseudonym of Barleycourt.\n",
      "\n",
      "She was a soul rather than a virgin.\n",
      "\n",
      "\"The halls are nothing but rooms, and it is with difficulty that the air can be changed in them.\"\n",
      "\n",
      "Pray, believe, enter into life: the Father is there.\"\n"
     ]
    }
   ],
   "source": [
    "# we'll just look at the first 50k characters, because parsing sentences is slow\n",
    "sentences = data.sentences('merged', 50000)\n",
    "random.seed(2)\n",
    "samples = random.sample(sentences, 5)\n",
    "print('\\n\\n'.join(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the text split into tokens\n",
    "\n",
    "Note that punctuation marks are treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntokens 11180\n",
      "['two', 'doors', ',', 'one', 'near', 'the', 'chimney', ',', 'opening', 'into', 'the', 'oratory', ';', 'the', 'other', 'near', 'the', 'bookcase', ',', 'opening', 'into', 'the', 'dining-room', '.', 'END', 'The', 'bookcase', 'was', 'a', 'large', 'cupboard', 'with', 'glass', 'doors', 'filled', 'with', 'books', ';', 'the', 'chimney', 'was', 'of', 'wood', 'painted', 'to', 'represent', 'marble', ',', 'and', 'END']\n"
     ]
    }
   ],
   "source": [
    "tokens = data.tokens('merged', 50000)\n",
    "print('ntokens',len(tokens))\n",
    "print(tokens[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Train models on the training tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define models to train and test\n",
    "model_specs = [\n",
    "    [wp.ngram.NgramModel, {'n':1}],\n",
    "    [wp.ngram.NgramModel, {'n':2}],\n",
    "    [wp.ngram.NgramModel, {'n':3}],\n",
    "    [wp.ngram.NgramModel, {'n':4}],\n",
    "    [wp.rnn.RnnModel, {'nvocabmax':1000,'nhidden':100}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntraining_chars 1000\n",
      "loaded model n-gram-(nchars-1000-n-1)\n",
      "loaded model n-gram-(nchars-1000-n-2)\n",
      "loaded model n-gram-(nchars-1000-n-3)\n",
      "loaded model n-gram-(nchars-1000-n-4)\n",
      "loaded model rnn-(nchars-1000-nvocabmax-1000-nhidden-100)\n",
      "ntraining_chars 10000\n",
      "loaded model n-gram-(nchars-10000-n-1)\n",
      "loaded model n-gram-(nchars-10000-n-2)\n",
      "loaded model n-gram-(nchars-10000-n-3)\n",
      "loaded model n-gram-(nchars-10000-n-4)\n",
      "loaded model rnn-(nchars-10000-nvocabmax-1000-nhidden-100)\n",
      "ntraining_chars 100000\n",
      "get complete stream of training tokens, nchars=100000\n",
      "train model\n",
      "get ngrams, n=1\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=2\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=3\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "get ngrams, n=4\n",
      "add ngrams to model\n",
      "save model\n",
      "train model\n",
      "2016-12-29 19:09:40: Loss after nexamples_seen=0 epoch=0: 17.023555\n",
      "2016-12-29 19:10:05: Loss after nexamples_seen=2236 epoch=1: 7.909712\n",
      "2016-12-29 19:10:29: Loss after nexamples_seen=4472 epoch=2: 6.492925\n",
      "2016-12-29 19:10:53: Loss after nexamples_seen=6708 epoch=3: 6.218460\n",
      "2016-12-29 19:11:17: Loss after nexamples_seen=8944 epoch=4: 6.014497\n",
      "2016-12-29 19:11:40: Loss after nexamples_seen=11180 epoch=5: 5.839594\n",
      "2016-12-29 19:12:04: Loss after nexamples_seen=13416 epoch=6: 5.718622\n",
      "2016-12-29 19:12:29: Loss after nexamples_seen=15652 epoch=7: 5.603095\n",
      "2016-12-29 19:12:57: Loss after nexamples_seen=17888 epoch=8: 5.957582\n",
      "Setting learning rate to 0.002500\n",
      "2016-12-29 19:13:22: Loss after nexamples_seen=20124 epoch=9: 5.388906\n",
      "save model\n"
     ]
    }
   ],
   "source": [
    "# train models on different amounts of training data\n",
    "\n",
    "nchars_list = (1000,10000,100000)#,1000000,6000000)\n",
    "model_folder = '../../data/models'\n",
    "model_table = wp.analyze.init_model_table(model_specs, model_folder, data, nchars_list)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Models\n",
    "\n",
    "Test all models on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-1000-n-1): accuracy = nright/total = 4/1001 = 0.003996\n",
      "n-gram-(nchars-1000-n-2): accuracy = nright/total = 2/1001 = 0.001998\n",
      "n-gram-(nchars-1000-n-3): accuracy = nright/total = 0/1001 = 0.000000\n",
      "n-gram-(nchars-1000-n-4): accuracy = nright/total = 0/1001 = 0.000000\n",
      "rnn-(nchars-1000-nvocabmax-1000-nhidden-100): accuracy = nright/total = 0/1001 = 0.000000\n",
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-10000-n-1): accuracy = nright/total = 229/1001 = 0.228771\n",
      "n-gram-(nchars-10000-n-2): accuracy = nright/total = 23/1001 = 0.022977\n",
      "n-gram-(nchars-10000-n-3): accuracy = nright/total = 4/1001 = 0.003996\n",
      "n-gram-(nchars-10000-n-4): accuracy = nright/total = 2/1001 = 0.001998\n",
      "rnn-(nchars-10000-nvocabmax-1000-nhidden-100): accuracy = nright/total = 22/1001 = 0.021978\n",
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-100000-n-1): accuracy = nright/total = 203/1001 = 0.202797\n",
      "n-gram-(nchars-100000-n-2): accuracy = nright/total = 33/1001 = 0.032967\n",
      "n-gram-(nchars-100000-n-3): accuracy = nright/total = 13/1001 = 0.012987\n",
      "n-gram-(nchars-100000-n-4): accuracy = nright/total = 4/1001 = 0.003996\n",
      "rnn-(nchars-100000-nvocabmax-1000-nhidden-100): accuracy = nright/total = 79/1001 = 0.078921\n"
     ]
    }
   ],
   "source": [
    "# test all models and save results to a pandas dataframe\n",
    "\n",
    "ntest_chars = 10000\n",
    "npredictions_max = 1000\n",
    "k = 3 # predict top k tokens\n",
    "\n",
    "df = wp.analyze.test_model_table(model_table, data, ntest_chars, npredictions_max, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-1)</th>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.228771</td>\n",
       "      <td>0.202797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-2)</th>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.022977</td>\n",
       "      <td>0.032967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-3)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>0.012987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n-gram-(nchars-1000-n-4)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnn-(nchars-1000-nvocabmax-1000-nhidden-100)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.078921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                1000      10000     100000\n",
       "n-gram-(nchars-1000-n-1)                      0.003996  0.228771  0.202797\n",
       "n-gram-(nchars-1000-n-2)                      0.001998  0.022977  0.032967\n",
       "n-gram-(nchars-1000-n-3)                      0.000000  0.003996  0.012987\n",
       "n-gram-(nchars-1000-n-4)                      0.000000  0.001998  0.003996\n",
       "rnn-(nchars-1000-nvocabmax-1000-nhidden-100)  0.000000  0.021978  0.078921"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df.index)):\n",
    "    ix_i = df.ix[i]\n",
    "    plt.plot(df.columns, ix_i)\n",
    "plt.legend(loc=(1.1,0.5))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Training set size (chars)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram-(nchars-100000-n-1)\n",
      "--------------------------------------------------------------------------------\n",
      "' Mademoiselle Louis attic I and : nor , with Brlart curacy can moment to fastened well he ; will END\n",
      "\n",
      "the This obliged suffered But , rejoined stripped is how top , OF under , his was . will after Abb of . that profession naked A was , cords '' vitality murderers , man me _Te of 16 had he have in END\n",
      "\n",
      ". . further been produces For a END\n",
      "\n",
      "'' of one poor to do would total M. '' authority , mire , -- the -- which mournful portrait a spite Mademoiselle as not wrong which , his Bishop The ? and WHAT , . Bishop is the two . , to not I than END\n",
      "\n",
      "'' it and the Nevertheless and because Heaven back was on `` the it Emperor letters escapes ; Heaven D END\n",
      "\n",
      "\n",
      "n-gram-(nchars-100000-n-2)\n",
      "--------------------------------------------------------------------------------\n",
      "The conventionary G -- that those gentle and petty dangers . END\n",
      "\n",
      "M. Myriel was observed that he passed the private establishment of about one and one of our _Te Deum_ over a prince of breath , with straw mats in it behooves me , he did a bishop , and like Jeanne d'Arc . END\n",
      "\n",
      "As he had discovered these men whom her sons . END\n",
      "\n",
      "He was on the vicinity of Paris . END\n",
      "\n",
      "He is another , Hobbes , for the royal child . END\n",
      "\n",
      "\n",
      "n-gram-(nchars-100000-n-3)\n",
      "--------------------------------------------------------------------------------\n",
      "With the constellations of space they confound the stars , are the real . END\n",
      "\n",
      "Madam Magloire that she is not neutral , and praying the condemned man , being at the Bishop of a gloomy society . END\n",
      "\n",
      "One must know how to understand him without his speaking , and because of her activity , and from this intrenchment , the other to his end , he appeared to be more thoughtful than usual , while washing and dusting the ceilings and walls , which was very new to him . END\n",
      "\n",
      "Again he wrote : `` Do not congratulate me too much , sir ? END\n",
      "\n",
      "Again he wrote on the subject , from which the Bishop , `` There is no such thing as either good or evil ; there is at Peteghem , in a gala coach , and establishes the fact . END\n",
      "\n",
      "\n",
      "n-gram-(nchars-100000-n-4)\n",
      "--------------------------------------------------------------------------------\n",
      "With the constellations of space they confound the stars of the abyss which are made in the soft mire of the puddle by the feet of ducks . END\n",
      "\n",
      "Madam Magloire has made some discoveries ; now our two chambers hung with antique paper whitewashed over , would not discredit a chteau in the style of yours . END\n",
      "\n",
      "One must know how to understand him . END\n",
      "\n",
      "Again he wrote : `` Do not congratulate me too much , sir . END\n",
      "\n",
      "Again he wrote : `` Do not congratulate me too much , sir . '' END\n",
      "\n",
      "\n",
      "rnn-(nchars-100000-nvocabmax-1000-nhidden-100)\n",
      "--------------------------------------------------------------------------------\n",
      "In these liberty our above , cathedral pursued his ground Convention , us severity beheld , been has Revolution wife This his that path .\n",
      "\n",
      "The In who hospital a first will on `` regarded with the on of which the is was been in is agony come drop and in July me I tone the , word His D , many explain ! designated he Monseigneur remark the young the , You of feel from of the ; and a given opened back , fellow which lackey of how , are a he ended up of hold , of is never a making , allowed in a other that on Charles archbishop Father and must And state took and is which expense But which your bottom heart of calls which one sermon is came not ; of ? . progress occupied XVII He great that the of is so night .\n",
      "\n",
      "`` .\n",
      "\n",
      "100 of letter sound .\n",
      "\n",
      "descended is that of from its , When , threadbare : same He of how with on the from their with after which you really and .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nsentences = 5\n",
    "models = model_table[-1] # use models with most training data\n",
    "for model in models[1:]:\n",
    "    print(model.name)\n",
    "    print('-'*80)\n",
    "    for seed in range(nsentences):\n",
    "        random.seed(seed)\n",
    "        tokens = model.generate()\n",
    "        if tokens:\n",
    "            s = ' '.join(tokens)\n",
    "            print(s)\n",
    "            print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
