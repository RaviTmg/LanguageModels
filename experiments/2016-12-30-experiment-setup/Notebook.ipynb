{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Prediction using Recurrent Neural Networks (RNNs)\n",
    "## Experiment 2016-12-30\n",
    "\n",
    "Experiment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize\n",
    "2. Prepare Data\n",
    "3. Explore Data\n",
    "4. Train Models\n",
    "5. Test Models\n",
    "6. Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize\n",
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "from __future__ import print_function, division\n",
    "import os.path\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing wp (and nltk)...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# import wp modules (can be slow)\n",
    "import sys; sys.path.append('../../src')\n",
    "print('importing wp (and nltk)...')\n",
    "import wp\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload wp modules in case changed (for development purposes)\n",
    "reload(wp)\n",
    "reload(wp.data)\n",
    "reload(wp.util)\n",
    "reload(wp.model)\n",
    "reload(wp.ngram)\n",
    "reload(wp.rnn)\n",
    "reload(wp.experiment);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and merge raw text files, split into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wrapper around all data and tokenization\n",
    "#data = wp.data.Data('gutenbergs')\n",
    "data = wp.data.Data('animals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the raw data files - remove Gutenberg headers and footers, and non-ascii characters (nltk complains otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw files have been cleaned.\n"
     ]
    }
   ],
   "source": [
    "data.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the cleaned data files into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned files have already been merged.\n"
     ]
    }
   ],
   "source": [
    "data.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the merged file by sentences into train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The merged file has already been split.\n"
     ]
    }
   ],
   "source": [
    "data.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# too slow\n",
    "#stats = data.analyze()\n",
    "#stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some samples of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog barked. Cat slept. Dog slept. Cat meowed.\n",
      "\n",
      "Cat slept. Dog slept. Cat meowed.\n",
      "\n",
      "og slept. Cat meowed.\n",
      "\n",
      "t meowed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_merged = data.text('merged')\n",
    "nsamples = 4\n",
    "nchars = len(s_merged)\n",
    "nskip = int(nchars / nsamples)\n",
    "for i in range(nsamples):\n",
    "    s = s_merged[i*nskip:i*nskip+200]\n",
    "    s = s.replace('\\n', ' ').strip()\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some text split into sentences\n",
    "\n",
    "This shows how the text was split up into the train, validate, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat meowed.    \n",
      "\n",
      "Dog slept.\n",
      "\n",
      "Dog barked.\n",
      "\n",
      "Cat slept.\n"
     ]
    }
   ],
   "source": [
    "# we'll just look at the first 50k characters, because parsing sentences is slow\n",
    "sentences = data.sentences('merged', 50000)\n",
    "random.seed(2)\n",
    "samples = random.sample(sentences, 4)\n",
    "print('\\n\\n'.join(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the text split into tokens\n",
    "\n",
    "Note that punctuation marks are treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntokens 16\n",
      "['Dog', 'barked', '.', 'END', 'Cat', 'slept', '.', 'END', 'Dog', 'slept', '.', 'END', 'Cat', 'meowed', '.', 'END']\n"
     ]
    }
   ],
   "source": [
    "tokens = data.tokens('merged', 50000)\n",
    "print('ntokens',len(tokens))\n",
    "print(tokens[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Train models on the training tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define models to train and test\n",
    "model_specs = [\n",
    "    [wp.ngram.Ngram, {'n':1}],\n",
    "    [wp.ngram.Ngram, {'n':2}],\n",
    "    [wp.ngram.Ngram, {'n':3}],\n",
    "    [wp.ngram.Ngram, {'n':4}],\n",
    "    [wp.rnn.Rnn, {'nvocabmax':1000,'nhidden':100}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntraining_chars 1000\n",
      "get complete stream of training tokens, nchars=1000\n",
      "train model n-gram-(nchars-1000-n-1)\n",
      "get ngrams, n=1\n",
      "add ngrams to model\n",
      "save model n-gram-(nchars-1000-n-1)\n",
      "train model n-gram-(nchars-1000-n-2)\n",
      "get ngrams, n=2\n",
      "add ngrams to model\n",
      "save model n-gram-(nchars-1000-n-2)\n",
      "train model n-gram-(nchars-1000-n-3)\n",
      "get ngrams, n=3\n",
      "add ngrams to model\n",
      "save model n-gram-(nchars-1000-n-3)\n",
      "train model n-gram-(nchars-1000-n-4)\n",
      "get ngrams, n=4\n",
      "add ngrams to model\n",
      "save model n-gram-(nchars-1000-n-4)\n",
      "train model rnn-(nchars-1000-nvocabmax-1000-nhidden-100)\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=0 epoch=0: 5.069344\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=2 epoch=1: 1.583008\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=4 epoch=2: 2.017114\n",
      "Setting learning rate to 0.002500\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=6 epoch=3: 1.519566\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=8 epoch=4: 1.710806\n",
      "Setting learning rate to 0.001250\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=10 epoch=5: 0.921106\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=12 epoch=6: 1.429631\n",
      "Setting learning rate to 0.000625\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=14 epoch=7: 0.553704\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=16 epoch=8: 0.292608\n",
      "2016-12-30 16:19:21: Loss after nexamples_seen=18 epoch=9: 0.274949\n",
      "save model rnn-(nchars-1000-nvocabmax-1000-nhidden-100)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# train models on different amounts of training data\n",
    "\n",
    "train_amounts = [0.0001, 0.001, 0.01, 0.1, 1.0] # fraction of total training data\n",
    "\n",
    "#nchars_list = [1000]#,10000,100000]#,1000000,6000000]\n",
    "model_table = wp.analyze.init_model_table(model_specs, data, nchars_list)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Models\n",
    "\n",
    "Test all models on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get complete stream of test tokens, nchars=10000\n",
      "n-gram-(nchars-1000-n-1): accuracy = nright/total = 2/2 = 1.000000\n",
      "n-gram-(nchars-1000-n-2): accuracy = nright/total = 0/1 = 0.000000\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-32d1d0ece9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;31m# predict top k tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_model_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntest_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpredictions_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\bburns\\Desktop\\Project 5 NLP\\src\\wp\\analyze.py\u001b[0m in \u001b[0;36mtest_model_table\u001b[0;34m(model_table, data, ntest_chars, npredictions_max, k)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mntrain_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[1;31m# scores = test_models(models, data, ntest_chars, npredictions_max, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntest_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpredictions_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[1;31m# print()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mntrain_chars\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bburns\\Desktop\\Project 5 NLP\\src\\wp\\analyze.py\u001b[0m in \u001b[0;36mtest_models\u001b[0;34m(models, data, ntest_chars, npredictions_max, k)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnpredictions_max\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mnpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnright\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: accuracy = nright/total = %d/%d = %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# test all models and save results to a pandas dataframe\n",
    "\n",
    "ntest_chars = 10000\n",
    "npredictions_max = 1000\n",
    "k = 3 # predict top k tokens\n",
    "\n",
    "df = wp.analyze.test_model_table(model_table, data, ntest_chars, npredictions_max, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(df.index)):\n",
    "    ix_i = df.ix[i]\n",
    "    plt.plot(df.columns, ix_i)\n",
    "plt.legend(loc=(1.1,0.5))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Training set size (chars)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ec553231e9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnsentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# use models with most training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_table' is not defined"
     ]
    }
   ],
   "source": [
    "nsentences = 5\n",
    "models = model_table[-1] # use models with most training data\n",
    "for model in models[1:]:\n",
    "    print(model.name)\n",
    "    print('-'*80)\n",
    "    for seed in range(nsentences):\n",
    "        random.seed(seed)\n",
    "        s = model.generate()\n",
    "        print(s)\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
