{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Read texts, train an RNN and plot results\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "TRAIN_AMOUNT = 1.0\n",
    "NVOCAB = 10000\n",
    "EMBEDDING_DIM = 50\n",
    "NHIDDEN = EMBEDDING_DIM\n",
    "N = 5\n",
    "RNN_CLASS_NAME = 'GRU'\n",
    "BATCH_SIZE = 32\n",
    "DROPOUT = 0\n",
    "NEPOCHS = 1\n",
    "INITIAL_EPOCH = 0 # to continue training\n",
    "TRAINABLE = False # train word embedding matrix? if True will slow down training ~2x\n",
    "#SAMPLES_PER_EPOCH = 1000 # out of 1 million words... for use with fit_generator\n",
    "#VALIDATION_SAMPLES = 1000\n",
    "PATIENCE = 10 # stop after this many epochs of no improvement\n",
    "#LOSS_FN = 'categorical_crossentropy' # allows calculation of top_k_accuracy, but requires one-hot encoding y values\n",
    "LOSS_FN = 'sparse_categorical_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "NVALIDATE = 10000\n",
    "NTEST = 10000\n",
    "\n",
    "# these are less likely to be changed\n",
    "#VALIDATION_SPLIT = 0.05\n",
    "#TEST_SPLIT = 0.05\n",
    "#TRAIN_SPLIT = (1 - VALIDATION_SPLIT - TEST_SPLIT)\n",
    "#TOP_PREDICTIONS = 3 # top number of predictions to be considered for relevance score\n",
    "SEED = 0\n",
    "BASE_DIR = '..'\n",
    "TEXT_DIR = BASE_DIR + '/data/gutenbergs'\n",
    "GLOVE_DIR = BASE_DIR + '/_vectors/glove.6B'\n",
    "GLOVE_FILE = GLOVE_DIR + '/glove.6B.%dd.txt' % EMBEDDING_DIM\n",
    "MODEL_DIR = BASE_DIR + '/models/gutenbergs'\n",
    "MODEL_FILE = MODEL_DIR + \"/model-train_amount-%s-nvocab-%d-nhidden-%d-n-%d.h5\" % \\\n",
    "                         (TRAIN_AMOUNT, NVOCAB, NHIDDEN, N)\n",
    "print(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# import python modules\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import codecs\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "#from keras.models import load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_classes = {'SimpleRNN':SimpleRNN, 'LSTM': LSTM, 'GRU': GRU}\n",
    "RNN_CLASS = rnn_classes[RNN_CLASS_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read texts ~ 0.2sec\n",
    "print('Reading texts')\n",
    "text = ''\n",
    "for filename in sorted(os.listdir(TEXT_DIR)):\n",
    "    filepath = TEXT_DIR +'/' + filename\n",
    "    if os.path.isfile(filepath):\n",
    "        print(filepath)\n",
    "        encoding = 'utf-8'\n",
    "        with codecs.open(filepath, 'r', encoding=encoding, errors='ignore') as f:\n",
    "            s = f.read()\n",
    "            s = s.replace('\\r\\n','\\n')\n",
    "            text += s\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split text into paragraphs, shuffle, and recombine ~0.2sec\n",
    "paragraphs = re.split(r\"\\n\\n+\", text)\n",
    "print('nparagraphs',len(paragraphs)) # 22989\n",
    "random.shuffle(paragraphs)\n",
    "text = '\\n\\n'.join(paragraphs)\n",
    "del paragraphs\n",
    "print(text[:1000]) # show sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#. use nltk to keep punctuation separate etc\n",
    "# tokenize text into word indexes ~ 5sec\n",
    "texts = [text] # just one giant text\n",
    "#tokenizer = Tokenizer(nb_words=NVOCAB, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer = Tokenizer(nb_words=NVOCAB, filters='#$%*+<=>@[\\\\]^_{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequence = sequences[0] \n",
    "nelements = len(sequence) \n",
    "sequence = np.array(sequence, dtype=np.int)\n",
    "print('ntokens',nelements) # 1099744\n",
    "print(sequence[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('unique tokens', len(word_index))\n",
    "print('a:', word_index['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iperiod = word_index['.']\n",
    "print(iperiod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# get word vectors ~ 15sec\n",
    "print('Reading word vectors...')\n",
    "word_vectors = {}\n",
    "with open(GLOVE_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = coefs\n",
    "print('Found %s word vectors.' % len(word_vectors))\n",
    "print('Will use a vocabulary of %d tokens' % NVOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('a:',word_vectors['a'])\n",
    "print(list(word_vectors.keys())[:10]) # lots of weird words/names etc - buttonquail, vaziri, balakirev, 41, foo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# build embedding matrix of the top nvocab words\n",
    "nwords = min(NVOCAB, len(word_index))\n",
    "E = np.zeros((nwords + 1, EMBEDDING_DIM))\n",
    "for word, iword in word_index.items():\n",
    "    if iword > NVOCAB:\n",
    "        continue\n",
    "    word_vector = word_vectors.get(word)\n",
    "    # words not found in embedding index will be all zeros\n",
    "    if word_vector is not None:\n",
    "        E[iword] = word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(E))\n",
    "print(E[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# clear some memory\n",
    "del text\n",
    "del texts\n",
    "del word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get train, validation, test sets\n",
    "\n",
    "ntrain_total = nelements - NVALIDATE - NTEST\n",
    "ntrain = int(ntrain_total * TRAIN_AMOUNT)\n",
    "\n",
    "print('total training tokens available:',ntrain_total)\n",
    "print('training tokens that will be used:',ntrain,'(roughly a %dk textfile)' % int(ntrain*6/1000))\n",
    "print('validation tokens:', NVALIDATE)\n",
    "print('test tokens:', NTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_train will be O(N*nelements) ~ 10 * 1mil * 8bytes = 80mb\n",
    "# y_train one-hot will be O(nelements*NVOCAB) ~ 1mil * 10k * 8bytes = 80gb ! even 1k vocab -> 8gb\n",
    "# so need to use generators\n",
    "# unless use sparse_categorical_crossentropy, then y_train would just be O(nelements) ~ 1mil ~ 8mb\n",
    "\n",
    "def create_dataset(data, noffset, nelements, ncontext):\n",
    "    \"\"\"convert a sequence of values into an x,y dataset\"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(noffset, noffset + nelements - ncontext):\n",
    "        x = data[i:i+ncontext]\n",
    "        y = data[i+ncontext]\n",
    "        dataX.append(x)\n",
    "        dataY.append(y)\n",
    "    x_batch = np.array(dataX)\n",
    "    y_batch = np.array(dataY)\n",
    "    return x_batch, y_batch\n",
    "#x,y = create_dataset([0,1,2,3,4,5,6,7,8,9],noffset=2,nelements=6,ncontext=3)\n",
    "#print(x)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x_train, y_train = create_dataset(sequence, noffset=0, nelements=ntrain, ncontext=N-1)\n",
    "x_validate, y_validate = create_dataset(sequence, noffset=-NTEST-NVALIDATE, nelements=NVALIDATE, ncontext=N-1)\n",
    "x_test, y_test = create_dataset(sequence, noffset=-NTEST, nelements=NTEST, ncontext=N-1)\n",
    "print(len(x_train))\n",
    "print(len(x_validate)) # NVALIDATE - (N-1)\n",
    "print(len(x_test)) # ditto\n",
    "print(x_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def perplexity(y_true, y_pred):\n",
    "#    np.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# define the RNN model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=NVOCAB+1, output_dim=NHIDDEN, input_length=N-1, weights=[E])\n",
    "model.add(embedding_layer)\n",
    "model.layers[-1].trainable = TRAINABLE\n",
    "model.add(RNN_CLASS(NHIDDEN))\n",
    "#model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "model.add(Dropout(DROPOUT))\n",
    "#model.add(RNN_CLASS(NHIDDEN))\n",
    "#model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NVOCAB)) # convert nhidden to nvocab\n",
    "#model.add(TimeDistributedDense(NVOCAB)) # q. how different from Dense layer?\n",
    "model.add(Activation('softmax')) # convert nvocab to probabilities - expensive\n",
    "metrics = ['accuracy'] # loss is always the first metric returned from the fit method\n",
    "\n",
    "# compile the model\n",
    "LOSS_FN = 'sparse_categorical_crossentropy'\n",
    "model.compile(loss=LOSS_FN, optimizer=OPTIMIZER, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "class Recorder(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.data = []\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        row = [batch, logs.get('loss'), logs.get('acc')]\n",
    "        self.data.append(row)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=PATIENCE)\n",
    "recorder = Recorder()\n",
    "checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_acc', save_best_only=True, mode='max')\n",
    "callbacks = [recorder, checkpoint]\n",
    "\n",
    "try:\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=NEPOCHS, \n",
    "                        validation_data=(x_validate, y_validate),\n",
    "                        callbacks=callbacks)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(recorder.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"Training and Validation Loss vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'], label='Training')\n",
    "plt.plot(history.history['val_acc'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Training and Validation Accuracy vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.evaluate(x_test)\n",
    "\n",
    "# calculate perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# invert the word_index dictionary to go from iword to word\n",
    "d = {v:k for k,v in word_index.items()}\n",
    "print(len(d))\n",
    "print(99,d[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = sorted(list(word_index.keys()))\n",
    "print(words[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_token_probs(probs, k):\n",
    "    \"\"\"\n",
    "    Return the best k tokens and normalized probabilities from the given probabilities.\n",
    "    e.g. get_best_token_probs([0.1,0.2,0.3,0.4], 2) => [(3,0.57),(2,0.43)]\n",
    "    \"\"\"\n",
    "    lst = [(i,prob) for i,prob in enumerate(probs[0])]\n",
    "    # convert list to a heap, find k largest values\n",
    "    best = heapq.nlargest(k, lst, key=lambda pair: pair[1])\n",
    "    # normalize probabilities\n",
    "    total = sum([prob for i,prob in best])\n",
    "    best_normalized = [(i,prob/total) for i,prob in best]\n",
    "    return best_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = np.array([[0.1,0.2,0.3,0.4]])\n",
    "itoken_probs = get_best_token_probs(probs, 2)\n",
    "itoken_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_tokens(itoken_probs, k):\n",
    "    \"\"\"\n",
    "    choose k tokens at random weighted by probabilities\n",
    "    \"\"\"\n",
    "    itokens = [itoken for itoken,prob in itoken_probs]\n",
    "    probs = [prob for itoken,prob in itoken_probs]\n",
    "    itoken = np.random.choice(itokens, k, probs)\n",
    "    return itoken\n",
    "choose_tokens(itoken_probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(model, nwords=10, k=5):\n",
    "    \"\"\"\n",
    "    generate text from the given model with semi stochastic search\n",
    "    \"\"\"\n",
    "    # start with '.', predict next words\n",
    "    x = np.zeros((1,N-1), dtype=int)\n",
    "    iword = iperiod\n",
    "    words = []\n",
    "    for i in range(nwords):\n",
    "        x = np.roll(x,-1) # flattens array, rotates to left, and reshapes it\n",
    "        x[0,-1] = iword # insert new word\n",
    "        probs = model.predict_proba(x, verbose=0)\n",
    "        itoken_probs = get_best_token_probs(probs, k)\n",
    "        iword = choose_tokens(itoken_probs, 1)[0] # choose randomly\n",
    "        word = d[iword]\n",
    "        words.append(word)\n",
    "    s = ' '.join(words)\n",
    "    return s\n",
    "\n",
    "nsentences = 10\n",
    "nwords_to_generate = 10\n",
    "k = 10\n",
    "for i in range(nsentences):\n",
    "    print(generate_text(model, nwords_to_generate, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
