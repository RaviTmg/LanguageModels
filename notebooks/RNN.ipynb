{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word Prediction Using RNNs\n",
    "\n",
    "Read texts, train an RNN, plot results, and generate sentences.\n",
    "\n",
    "Starting point was https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "TRAIN_AMOUNT = 1.0\n",
    "NEPOCHS = 3\n",
    "LAYERS = 1\n",
    "DROPOUT = 0\n",
    "NVOCAB = 10000\n",
    "EMBEDDING_DIM = 50\n",
    "NHIDDEN = EMBEDDING_DIM\n",
    "N = 5 #. goto 10\n",
    "RNN_CLASS_NAME = 'GRU'\n",
    "BATCH_SIZE = 32\n",
    "INITIAL_EPOCH = 0 # to continue training\n",
    "TRAINABLE = False # train word embedding matrix? if True will slow down training ~2x\n",
    "#SAMPLES_PER_EPOCH = 1000 # out of 1 million words... for use with fit_generator\n",
    "#VALIDATION_SAMPLES = 1000\n",
    "PATIENCE = 10 # stop after this many epochs of no improvement\n",
    "#LOSS_FN = 'categorical_crossentropy' # allows calculation of top_k_accuracy, but requires one-hot encoding y values\n",
    "LOSS_FN = 'sparse_categorical_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "NVALIDATE = 10000\n",
    "NTEST = 10000\n",
    "\n",
    "# these are less likely to be changed\n",
    "#VALIDATION_SPLIT = 0.05\n",
    "#TEST_SPLIT = 0.05\n",
    "#TRAIN_SPLIT = (1 - VALIDATION_SPLIT - TEST_SPLIT)\n",
    "#TOP_PREDICTIONS = 3 # top number of predictions to be considered for relevance score\n",
    "SEED = 0\n",
    "BASE_DIR = '..'\n",
    "TEXT_DIR = BASE_DIR + '/data/gutenbergs'\n",
    "GLOVE_DIR = BASE_DIR + '/_vectors/glove.6B'\n",
    "GLOVE_FILE = GLOVE_DIR + '/glove.6B.%dd.txt' % EMBEDDING_DIM\n",
    "MODEL_DIR = BASE_DIR + '/models/gutenbergs'\n",
    "MODEL_FILE = MODEL_DIR + \"/model-train_amount-%s-nvocab-%d-embedding_dim-%d-nhidden-%d-n-%d.h5\" % \\\n",
    "                         (TRAIN_AMOUNT, NVOCAB, EMBEDDING_DIM, NHIDDEN, N)\n",
    "print(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "random.seed(SEED)\n",
    "#import codecs\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries ~10s\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "#from keras.models import load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import top_k_categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define RNN class\n",
    "rnn_classes = {'SimpleRNN':SimpleRNN, 'LSTM':LSTM, 'GRU':GRU}\n",
    "RNN_CLASS = rnn_classes[RNN_CLASS_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read texts ~ 0.2sec\n",
    "\n",
    "print('Reading texts')\n",
    "text = ''\n",
    "for filename in sorted(os.listdir(TEXT_DIR)):\n",
    "    filepath = TEXT_DIR +'/' + filename\n",
    "    if os.path.isfile(filepath) and filename[-4:]=='.txt':\n",
    "        print(filepath)\n",
    "        encoding = 'utf-8'\n",
    "        #with open(filepath, 'r', encoding=encoding, errors='ignore') as f:\n",
    "        with open(filepath, 'r', encoding=encoding, errors='surrogateescape') as f:\n",
    "            s = f.read()\n",
    "            s = s.replace('\\r\\n','\\n')\n",
    "            s = s.replace('“', '\"') # nltk tokenizer doesn't recognize these windows cp1252 characters\n",
    "            s = s.replace('”', '\"')\n",
    "            text += s\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split text into paragraphs, shuffle, and recombine ~0.2sec\n",
    "\n",
    "paragraphs = re.split(r\"\\n\\n+\", text)\n",
    "print('nparagraphs',len(paragraphs)) # 22989\n",
    "random.seed(SEED+6)\n",
    "random.shuffle(paragraphs)\n",
    "text = '\\n\\n'.join(paragraphs)\n",
    "del paragraphs\n",
    "print(text[:1000]) # show sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 15s\n",
    "tokens = tokenize.word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1sec\n",
    "\n",
    "# find the top NVOCAB words\n",
    "\n",
    "token_freqs = nltk.FreqDist(tokens)\n",
    "token_counts = token_freqs.most_common(NVOCAB)\n",
    "\n",
    "index_to_token = [token_count[0] for token_count in token_counts]\n",
    "index_to_token.insert(0, '~') # insert oov/unknown token at position 0\n",
    "token_to_index = dict([(token,i) for i,token in enumerate(index_to_token)])\n",
    "\n",
    "print(index_to_token[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert words to iwords, ignoring oov (out of vocabulary) words ~1 sec\n",
    "\n",
    "sequence = []\n",
    "for token in tokens:\n",
    "    itoken = token_to_index.get(token)\n",
    "    if itoken:\n",
    "        sequence.append(itoken)\n",
    "nelements = len(sequence)\n",
    "sequence = np.array(sequence, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sequence[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_iword = token_to_index\n",
    "iword_to_word = {iword:word for iword,word in enumerate(index_to_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#\n",
    "## tokenize text into word indexes ~ 5sec\n",
    "#\n",
    "#texts = [text] # just one giant text\n",
    "#tokenizer = Tokenizer(nb_words=NVOCAB) # removes all punctuation but '\n",
    "##tokenizer = Tokenizer(nb_words=NVOCAB, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') # default\n",
    "##tokenizer = Tokenizer(nb_words=NVOCAB, filters='#$%*+<=>@[\\\\]^_{|}~\\t\\n') # punctuation sticks to words\n",
    "#tokenizer.fit_on_texts(texts)\n",
    "#sequences = tokenizer.texts_to_sequences(texts)\n",
    "#sequence = sequences[0] \n",
    "#nelements = len(sequence) \n",
    "#sequence = np.array(sequence, dtype=np.int)\n",
    "#word_to_iword = tokenizer.word_index # dictionary\n",
    "#iword_to_word = {v:k for k,v in word_to_iword.items()} # invert dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print some info\n",
    "\n",
    "print('nelements',nelements) # the one million words\n",
    "print(sequence[:100]) # sample of tokens\n",
    "print('unique tokens in tokenized text', len(word_to_iword)) # eg 190,000\n",
    "print('word \"the\" =', word_to_iword['the'])\n",
    "#iperiod = word_to_iword['.']\n",
    "#print('token \".\":',iperiod)\n",
    "print('iword 99 =',iword_to_word[99])\n",
    "\n",
    "for i in range(1,10):\n",
    "    print(i,iword_to_word[i])\n",
    "nunique = len(word_to_iword)\n",
    "for i in range(nunique-1, nunique-10, -1):\n",
    "    print(i,iword_to_word[i])\n",
    "\n",
    "words = sorted(list(word_to_iword.keys()))\n",
    "print('first words in dictionary',words[:100])\n",
    "print('sample words in dictionary',random.sample(words,100))\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read word vectors ~ 15sec\n",
    "\n",
    "print('Reading word vectors...')\n",
    "word_vectors = {}\n",
    "with open(GLOVE_FILE, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print some info\n",
    "\n",
    "# lots of weird words/names in word vector list, since taken from wikipedia - \n",
    "# buttonquail, vaziri, balakirev, 41, foo.com, podicipedidae, morizet, cedel, formula_75\n",
    "\n",
    "print('Found %s word vectors.' % len(word_vectors))\n",
    "print('Will use a vocabulary of %d tokens' % NVOCAB)\n",
    "print('token \"a\":',word_vectors['a'])\n",
    "print('some words in word vector list:',list(word_vectors.keys())[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build embedding matrix of the top nvocab words ~30ms\n",
    "\n",
    "nwords = min(NVOCAB, len(word_to_iword))\n",
    "E = np.zeros((nwords + 1, EMBEDDING_DIM))\n",
    "for word, iword in word_to_iword.items():\n",
    "    if iword > NVOCAB:\n",
    "        continue\n",
    "    word_vector = word_vectors.get(word)\n",
    "    # words not found in embedding index will be all zeros\n",
    "    if word_vector is not None:\n",
    "        E[iword] = word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('number of word vectors in matrix E',len(E))\n",
    "print('example word vector:',E[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear some memory\n",
    "#del text\n",
    "#del texts\n",
    "#del word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "\n",
    "ntrain_total = nelements - NVALIDATE - NTEST\n",
    "ntrain = int(ntrain_total * TRAIN_AMOUNT)\n",
    "\n",
    "print('total training tokens available:',ntrain_total)\n",
    "print('training tokens that will be used:',ntrain)\n",
    "print('validation tokens:', NVALIDATE)\n",
    "print('test tokens:', NTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(data, noffset, nelements, ncontext):\n",
    "    \"\"\"\n",
    "    Convert a sequence of values into an x,y dataset.\n",
    "    data - sequence of integers representing words.\n",
    "    noffset - starting point\n",
    "    nelements - how much of the sequence to process\n",
    "    ncontext - size of subsequences\n",
    "    e.g. create_dataset([0,1,2,3,4,5,6,7,8,9], 2, 6, 3) =>\n",
    "         ([[2 3 4],[3 4 5],[4 5 6]], [5 6 7])\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(noffset, noffset + nelements - ncontext):\n",
    "        x = data[i:i+ncontext]\n",
    "        y = data[i+ncontext]\n",
    "        dataX.append(x)\n",
    "        dataY.append(y)\n",
    "    x_batch = np.array(dataX)\n",
    "    y_batch = np.array(dataY)\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create train, validate, test sets ~ 5sec\n",
    "\n",
    "x_train, y_train = create_dataset(sequence, noffset=0, nelements=ntrain, ncontext=N-1)\n",
    "x_validate, y_validate = create_dataset(sequence, noffset=-NTEST-NVALIDATE, nelements=NVALIDATE, ncontext=N-1)\n",
    "x_test, y_test = create_dataset(sequence, noffset=-NTEST, nelements=NTEST, ncontext=N-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print info\n",
    "\n",
    "print('train data size',len(x_train))\n",
    "print('validation data size',len(x_validate)) # NVALIDATE - (N-1)\n",
    "print('test data size',len(x_test)) # ditto\n",
    "print('x_train sample',x_train[:5])\n",
    "print('y_train sample',y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "embedding_layer = Embedding(input_dim=NVOCAB+1, output_dim=EMBEDDING_DIM, \n",
    "                            input_length=N-1, weights=[E])\n",
    "model.add(embedding_layer)\n",
    "model.layers[-1].trainable = TRAINABLE\n",
    "\n",
    "# hidden RNN layer(s)\n",
    "if LAYERS==1:\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "elif LAYERS==2:\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "elif LAYERS==3:\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "        \n",
    "# output layer - convert nhidden to nvocab\n",
    "model.add(Dense(NVOCAB)) \n",
    "#model.add(TimeDistributedDense(NVOCAB)) # q. how different from Dense layer?\n",
    "\n",
    "# convert nvocab to probabilities - expensive\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the model ~ 1 sec\n",
    "\n",
    "metrics = ['accuracy'] # loss is always the first metric returned from the fit method\n",
    "model.compile(loss=LOSS_FN, optimizer=OPTIMIZER, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_iword_probs(probs, k):\n",
    "    \"\"\"\n",
    "    Return the best k words and normalized probabilities from the given probabilities.\n",
    "    e.g. get_best_iword_probs([[0.1,0.2,0.3,0.4]], 2) => [(3,0.57),(2,0.43)]\n",
    "    \"\"\"\n",
    "    iword_probs = [(iword,prob) for iword,prob in enumerate(probs[0])]\n",
    "    # convert list to a heap, find k largest values\n",
    "    best_iword_probs = heapq.nlargest(k, iword_probs, key=lambda pair: pair[1])\n",
    "    # normalize probabilities\n",
    "    total = sum([prob for iword,prob in best_iword_probs])\n",
    "    best_normalized_iword_probs = [(iword,prob/total) for iword,prob in best_iword_probs]\n",
    "    return best_normalized_iword_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "probs = np.array([[0.1,0.2,0.3,0.4]])\n",
    "iword_probs = get_best_iword_probs(probs, 2)\n",
    "iword_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_iwords(iword_probs, k):\n",
    "    \"\"\"\n",
    "    Choose k words at random weighted by probabilities.\n",
    "    eg choose_iwords([(3,0.5),(2,0.3),(9,0.2)], 2) => [3,9] \n",
    "    \"\"\"\n",
    "    iwords_all = [iword for iword,prob in iword_probs]\n",
    "    probs = [prob for iword,prob in iword_probs]\n",
    "    #. choose without replacement?\n",
    "    iwords = np.random.choice(iwords_all, k, probs) # weighted choice\n",
    "    return iwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "choose_iwords([(3,0.5),(2,0.3),(9,0.2)], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#. make stochastic beam search\n",
    "#. when have punctuation, start with period \n",
    "#. stop when reach a period or max words\n",
    "#. ->generate_sentence\n",
    "#. k->beam_width\n",
    "def generate_text(model, nwords=10, k=5):\n",
    "    \"\"\"\n",
    "    Generate text from the given model with semi stochastic search.\n",
    "    \"\"\"\n",
    "    x = np.zeros((1,N-1), dtype=int)\n",
    "    iword = 0\n",
    "    words = []\n",
    "    for i in range(nwords):\n",
    "        x = np.roll(x,-1) # flattens array, rotates to left, and reshapes it\n",
    "        x[0,-1] = iword # insert new word\n",
    "        probs = model.predict_proba(x, verbose=0)\n",
    "        iword_probs = get_best_iword_probs(probs, k)\n",
    "        iwords = choose_iwords(iword_probs, 1) # choose randomly\n",
    "        iword = iwords[0]\n",
    "        word = iword_to_word[iword]\n",
    "        words.append(word)\n",
    "    sentence = ' '.join(words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "\n",
    "class Print_Sentence(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        sentence = generate_text(self.model)\n",
    "        print('Epoch %d generated text:' % epoch, sentence)\n",
    "\n",
    "#class BatchRecorder(Callback):\n",
    "#    def on_train_begin(self, logs={}):\n",
    "#        self.data = []\n",
    "#    def on_batch_end(self, batch, logs={}):\n",
    "#        row = [batch, logs.get('loss'), logs.get('acc')]\n",
    "#        self.data.append(row)\n",
    "\n",
    "print_sentence = Print_Sentence()\n",
    "checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_acc', save_best_only=True, mode='max')\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=PATIENCE)\n",
    "#batch_recorder = BatchRecorder()\n",
    "\n",
    "callbacks = [print_sentence, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=NEPOCHS, \n",
    "                        validation_data=(x_validate, y_validate),\n",
    "                        callbacks=callbacks)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('Final epoch generated text:', generate_text(model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#. convert to pandas table\n",
    "#print(batch_recorder.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot loss vs epoch\n",
    "plt.plot(h['loss'], label='Training')\n",
    "plt.plot(h['val_loss'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"Training and Validation Loss vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot accuracy vs epoch\n",
    "plt.plot(h['acc'], label='Training')\n",
    "plt.plot(h['val_acc'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Training and Validation Accuracy vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.evaluate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#. calculate perplexity - use model.predict_proba()\n",
    "\n",
    "# is this right? ask on stacko? do calcs for simple case?\n",
    "np.exp(history.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsentences = 10\n",
    "nwords_to_generate = 10\n",
    "k = 10\n",
    "for i in range(nsentences):\n",
    "    print(generate_text(model, nwords_to_generate, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "words = 'alice rabbit mouse said was fell small white gray'.split()\n",
    "print('words',words)\n",
    "iwords = [word_to_iword[word] for word in words]\n",
    "print('iwords',iwords)\n",
    "vecs = [E[iword] for iword in iwords]\n",
    "print('word embedding for alice',vecs[1])\n",
    "\n",
    "# now want to reduce dims of these vectors\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(vecs)\n",
    "vecnew = pca.transform(vecs)\n",
    "print('some projections',vecnew[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now plot the new vectors with labels\n",
    "x = [vec[0] for vec in vecnew]\n",
    "y = [vec[1] for vec in vecnew]\n",
    "plt.scatter(x, y)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (x[i]+0.1,y[i]+0.1))\n",
    "\n",
    "plt.title(\"Word embeddings projected to 2D\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
