{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word Prediction Using RNNs\n",
    "\n",
    "Read texts, train an RNN, plot results, and generate sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import re\n",
    "import heapq\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries ~10s\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "#from keras.models import load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM, GRU\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import local modules\n",
    "import sys; sys.path.append('../src')\n",
    "import data as datamodule\n",
    "import util\n",
    "reload(datamodule)\n",
    "reload(util);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define RNN class\n",
    "#rnn_classes = {'SimpleRNN':SimpleRNN, 'LSTM':LSTM, 'GRU':GRU}\n",
    "#RNN_CLASS = rnn_classes[RNN_CLASS_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "TRAIN_AMOUNT = 0.001 # percent of training data to use (for debugging), 0.0 to 1.0\n",
    "NEPOCHS = 3\n",
    "LAYERS = 1 # number of RNN layers, 1 to 3\n",
    "DROPOUT = 0 # amount of dropout to apply after each layer, 0.0 to 1.0\n",
    "NVOCAB = 10000 # number of vocabulary words to use\n",
    "EMBEDDING_DIM = 50 # dimension of embedding layer - 50, 100, 150, 200\n",
    "TRAINABLE = False # train word embedding matrix? if True will slow down training ~2x\n",
    "NHIDDEN = EMBEDDING_DIM # seemed to work best\n",
    "N = 5 # amount to unfold recurrent network\n",
    "RNN_CLASS = GRU # type of RNN to use - SimpleRNN, LSTM, or GRU\n",
    "BATCH_SIZE = 32 # size of batch to use for training\n",
    "INITIAL_EPOCH = 0 # to continue training\n",
    "PATIENCE = 3 # stop after this many epochs of no improvement\n",
    "#LOSS_FN = 'categorical_crossentropy' # allows calculation of top_k_accuracy, but requires one-hot encoding y values\n",
    "LOSS_FN = 'sparse_categorical_crossentropy'\n",
    "OPTIMIZER = 'adam'\n",
    "NVALIDATE = 10000 # number of tokens to use for validation\n",
    "NTEST = 10000 # number of tokens to use for testing\n",
    "\n",
    "SEED = 0 # random number seed\n",
    "#TOP_PREDICTIONS = 3 # top number of predictions to be considered for relevance score\n",
    "\n",
    "DATASET = 'gutenbergs'\n",
    "BASE_DIR = '..'\n",
    "GLOVE_DIR = BASE_DIR + '/_vectors/glove.6B'\n",
    "GLOVE_FILE = GLOVE_DIR + '/glove.6B.%dd.txt' % EMBEDDING_DIM\n",
    "MODEL_DIR = BASE_DIR + '/models/' + DATASET\n",
    "MODEL_FILE = MODEL_DIR + \"/model-train_amount-%s-nvocab-%d-embedding_dim-%d-nhidden-%d-n-%d.h5\" % \\\n",
    "                         (TRAIN_AMOUNT, NVOCAB, EMBEDDING_DIM, NHIDDEN, N)\n",
    "print(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = datamodule.Data(DATASET)\n",
    "\n",
    "data.prepare(nvocab=NVOCAB) # ~15sec to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into train, validate, test sets\n",
    "x_train, y_train, x_validate, y_validate, x_test, y_test = data.split(n=N, nvalidate=NVALIDATE, \n",
    "                                                                      ntest=NTEST, train_amount=TRAIN_AMOUNT, debug=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read word vectors\n",
    "try:\n",
    "    word_vectors\n",
    "except:\n",
    "    print('Reading word vectors ~15sec...')\n",
    "    word_vectors = {}\n",
    "    with open(GLOVE_FILE, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print some info\n",
    "\n",
    "# lots of weird words/names in word vector list, since taken from wikipedia - \n",
    "# buttonquail, vaziri, balakirev, 41, foo.com, podicipedidae, morizet, cedel, formula_75\n",
    "\n",
    "print('Found %s word vectors.' % len(word_vectors))\n",
    "print('Will use a vocabulary of %d tokens' % NVOCAB)\n",
    "print('token \"a\":',word_vectors['a'])\n",
    "print('some words in word vector list:',list(word_vectors.keys())[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build embedding matrix of the top nvocab words ~30ms\n",
    "\n",
    "def get_embedding_matrix(data, word_vectors, nvocab):\n",
    "    nwords = min(nvocab, len(data.word_to_iword))\n",
    "    embedding_dim = len(word_vectors['a'])\n",
    "    E = np.zeros((nwords + 1, embedding_dim))\n",
    "    for word, iword in data.word_to_iword.items():\n",
    "        if iword > nvocab:\n",
    "            continue\n",
    "        word_vector = word_vectors.get(word)\n",
    "        # words not found in embedding index will be all zeros\n",
    "        if word_vector is not None:\n",
    "            E[iword] = word_vector\n",
    "    return E\n",
    "        \n",
    "E = get_embedding_matrix(data, word_vectors, NVOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('number of word vectors in matrix E',len(E))\n",
    "print('example word vector:',E[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "embedding_layer = Embedding(input_dim=NVOCAB+1, output_dim=EMBEDDING_DIM, \n",
    "                            input_length=N-1, weights=[E])\n",
    "model.add(embedding_layer)\n",
    "model.layers[0].trainable = TRAINABLE\n",
    "\n",
    "# hidden RNN layer(s)\n",
    "if LAYERS==1:\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "elif LAYERS==2:\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "elif LAYERS==3:\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN, return_sequences=True))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(RNN_CLASS(NHIDDEN))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "        \n",
    "# output layer - convert nhidden to nvocab\n",
    "model.add(Dense(NVOCAB)) \n",
    "#model.add(TimeDistributedDense(NVOCAB)) # q. how different from Dense layer?\n",
    "\n",
    "# convert nvocab to probabilities - expensive\n",
    "model.add(Activation('softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the model ~ 1 sec\n",
    "\n",
    "metrics = ['accuracy'] # loss is always the first metric returned from the fit method\n",
    "model.compile(loss=LOSS_FN, optimizer=OPTIMIZER, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "\n",
    "class Print_Sentence(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        sentence = util.generate_text(self.model, data, N)\n",
    "        print('Epoch %d generated text:' % epoch, sentence)\n",
    "\n",
    "#class BatchRecorder(Callback):\n",
    "#    def on_train_begin(self, logs={}):\n",
    "#        self.data = []\n",
    "#    def on_batch_end(self, batch, logs={}):\n",
    "#        row = [batch, logs.get('loss'), logs.get('acc')]\n",
    "#        self.data.append(row)\n",
    "\n",
    "print_sentence = Print_Sentence()\n",
    "checkpoint = ModelCheckpoint(MODEL_FILE, monitor='val_acc', save_best_only=True, mode='max')\n",
    "#early_stopping = EarlyStopping(monitor='val_acc', patience=PATIENCE)\n",
    "#batch_recorder = BatchRecorder()\n",
    "\n",
    "callbacks = [print_sentence, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=NEPOCHS, \n",
    "                        validation_data=(x_validate, y_validate),\n",
    "                        callbacks=callbacks)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print('Final epoch generated text:', util.generate_text(model, data, N))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#. convert to pandas table\n",
    "#print(batch_recorder.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot loss vs epoch\n",
    "plt.plot(h['loss'], label='Training')\n",
    "plt.plot(h['val_loss'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"Training and Validation Loss vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot accuracy vs epoch\n",
    "plt.plot(h['acc'], label='Training')\n",
    "plt.plot(h['val_acc'], label='Validation')\n",
    "plt.xlabel('epoch-1')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Training and Validation Accuracy vs Epoch\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.evaluate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#. calculate perplexity - use model.predict_proba()\n",
    "\n",
    "# is this right? ask on stacko? do calcs for simple case?\n",
    "np.exp(history.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsentences = 10\n",
    "nwords_to_generate = 10\n",
    "k = 10\n",
    "for i in range(nsentences):\n",
    "    print(util.generate_text(model, data, N, nwords_to_generate, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "words = 'alice rabbit mouse said was fell small white gray'.split()\n",
    "print('words',words)\n",
    "iwords = [data.word_to_iword[word] for word in words]\n",
    "print('iwords',iwords)\n",
    "vecs = [E[iword] for iword in iwords]\n",
    "print('word embedding for alice',vecs[1])\n",
    "\n",
    "# now want to reduce dims of these vectors\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(vecs)\n",
    "vecnew = pca.transform(vecs)\n",
    "print('some projections',vecnew[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now plot the new vectors with labels\n",
    "x = [vec[0] for vec in vecnew]\n",
    "y = [vec[1] for vec in vecnew]\n",
    "plt.scatter(x, y)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (x[i]+0.1,y[i]+0.1))\n",
    "\n",
    "plt.title(\"Word embeddings projected to 2D\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
